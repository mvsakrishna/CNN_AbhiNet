{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# 1. Build your own convolutional neural network using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4341475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbhiNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class AbhiNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3):\n",
    "        super(AbhiNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(256 * block.expansion, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def create_AbhiNet(num_classes=3):\n",
    "    return AbhiNet(BasicBlock, [2, 2, 2], num_classes)\n",
    "\n",
    "model = create_AbhiNet(num_classes=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c45b84",
   "metadata": {},
   "source": [
    "# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code) \n",
    "\n",
    "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
    "### (2) use custom dataloader for test dataset (return image tensor and file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50effdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.9940\n",
      "Epoch [1/20], Validation Loss: 1.0994, Accuracy: 39.00%\n",
      "Saving best weights at epoch 1\n",
      "Epoch [2/20], Training Loss: 0.9082\n",
      "Epoch [2/20], Validation Loss: 0.8738, Accuracy: 54.00%\n",
      "Saving best weights at epoch 2\n",
      "Epoch [3/20], Training Loss: 0.8224\n",
      "Epoch [3/20], Validation Loss: 0.8614, Accuracy: 55.50%\n",
      "Saving best weights at epoch 3\n",
      "Epoch [4/20], Training Loss: 0.7703\n",
      "Epoch [4/20], Validation Loss: 0.7383, Accuracy: 60.50%\n",
      "Saving best weights at epoch 4\n",
      "Epoch [5/20], Training Loss: 0.7130\n",
      "Epoch [5/20], Validation Loss: 1.1410, Accuracy: 51.00%\n",
      "Epoch [6/20], Training Loss: 0.7058\n",
      "Epoch [6/20], Validation Loss: 0.6825, Accuracy: 58.50%\n",
      "Saving best weights at epoch 6\n",
      "Epoch [7/20], Training Loss: 0.6507\n",
      "Epoch [7/20], Validation Loss: 0.6948, Accuracy: 64.00%\n",
      "Epoch [8/20], Training Loss: 0.6257\n",
      "Epoch [8/20], Validation Loss: 0.6104, Accuracy: 68.00%\n",
      "Saving best weights at epoch 8\n",
      "Epoch [9/20], Training Loss: 0.6260\n",
      "Epoch [9/20], Validation Loss: 0.6538, Accuracy: 67.00%\n",
      "Epoch [10/20], Training Loss: 0.6297\n",
      "Epoch [10/20], Validation Loss: 0.6249, Accuracy: 68.00%\n",
      "Epoch [11/20], Training Loss: 0.5987\n",
      "Epoch [11/20], Validation Loss: 0.6052, Accuracy: 70.50%\n",
      "Saving best weights at epoch 11\n",
      "Epoch [12/20], Training Loss: 0.6003\n",
      "Epoch [12/20], Validation Loss: 0.6023, Accuracy: 69.50%\n",
      "Saving best weights at epoch 12\n",
      "Epoch [13/20], Training Loss: 0.5719\n",
      "Epoch [13/20], Validation Loss: 0.5887, Accuracy: 70.00%\n",
      "Saving best weights at epoch 13\n",
      "Epoch [14/20], Training Loss: 0.5766\n",
      "Epoch [14/20], Validation Loss: 0.6086, Accuracy: 70.50%\n",
      "Epoch [15/20], Training Loss: 0.5746\n",
      "Epoch [15/20], Validation Loss: 0.6111, Accuracy: 72.00%\n",
      "Epoch [16/20], Training Loss: 0.5760\n",
      "Epoch [16/20], Validation Loss: 0.6187, Accuracy: 70.50%\n",
      "Epoch [17/20], Training Loss: 0.5652\n",
      "Epoch [17/20], Validation Loss: 0.6151, Accuracy: 70.50%\n",
      "Epoch [18/20], Training Loss: 0.5610\n",
      "Epoch [18/20], Validation Loss: 0.6033, Accuracy: 72.00%\n",
      "Epoch [19/20], Training Loss: 0.5648\n",
      "Epoch [19/20], Validation Loss: 0.6008, Accuracy: 72.50%\n",
      "Epoch [20/20], Training Loss: 0.5712\n",
      "Epoch [20/20], Validation Loss: 0.5971, Accuracy: 72.00%\n",
      "Training complete. Best validation loss: 0.5887\n",
      "File: 100.png, Predicted Class: 1\n",
      "File: 1621.png, Predicted Class: 0\n",
      "File: 1622.png, Predicted Class: 2\n",
      "File: 1623.png, Predicted Class: 2\n",
      "File: 1624.png, Predicted Class: 2\n",
      "File: 1625.png, Predicted Class: 1\n",
      "File: 1626.png, Predicted Class: 0\n",
      "File: 1627.png, Predicted Class: 0\n",
      "File: 1628.png, Predicted Class: 0\n",
      "File: 1629.png, Predicted Class: 1\n",
      "File: 1630.png, Predicted Class: 0\n",
      "File: 1631.png, Predicted Class: 1\n",
      "File: 1632.png, Predicted Class: 1\n",
      "File: 1633.png, Predicted Class: 1\n",
      "File: 1634.png, Predicted Class: 0\n",
      "File: 1635.png, Predicted Class: 2\n",
      "File: 1636.png, Predicted Class: 1\n",
      "File: 1637.png, Predicted Class: 2\n",
      "File: 1638.png, Predicted Class: 1\n",
      "File: 1639.png, Predicted Class: 0\n",
      "File: 1640.png, Predicted Class: 1\n",
      "File: 1641.png, Predicted Class: 2\n",
      "File: 1642.png, Predicted Class: 2\n",
      "File: 1643.png, Predicted Class: 0\n",
      "File: 1644.png, Predicted Class: 0\n",
      "File: 1645.png, Predicted Class: 1\n",
      "File: 1646.png, Predicted Class: 0\n",
      "File: 1647.png, Predicted Class: 0\n",
      "File: 1648.png, Predicted Class: 0\n",
      "File: 1649.png, Predicted Class: 0\n",
      "File: 1650.png, Predicted Class: 0\n",
      "File: 1651.png, Predicted Class: 0\n",
      "File: 1652.png, Predicted Class: 0\n",
      "File: 1653.png, Predicted Class: 0\n",
      "File: 1654.png, Predicted Class: 0\n",
      "File: 1655.png, Predicted Class: 0\n",
      "File: 1656.png, Predicted Class: 2\n",
      "File: 1657.png, Predicted Class: 0\n",
      "File: 1658.png, Predicted Class: 2\n",
      "File: 1659.png, Predicted Class: 1\n",
      "File: 1660.png, Predicted Class: 1\n",
      "File: 1661.png, Predicted Class: 2\n",
      "File: 1662.png, Predicted Class: 0\n",
      "File: 1663.png, Predicted Class: 0\n",
      "File: 1664.png, Predicted Class: 0\n",
      "File: 1665.png, Predicted Class: 0\n",
      "File: 1666.png, Predicted Class: 0\n",
      "File: 1667.png, Predicted Class: 1\n",
      "File: 1668.png, Predicted Class: 1\n",
      "File: 1669.png, Predicted Class: 0\n",
      "File: 1670.png, Predicted Class: 0\n",
      "File: 1671.png, Predicted Class: 0\n",
      "File: 1672.png, Predicted Class: 1\n",
      "File: 1673.png, Predicted Class: 0\n",
      "File: 1674.png, Predicted Class: 1\n",
      "File: 1675.png, Predicted Class: 2\n",
      "File: 1676.png, Predicted Class: 0\n",
      "File: 1677.png, Predicted Class: 2\n",
      "File: 1678.png, Predicted Class: 0\n",
      "File: 1679.png, Predicted Class: 1\n",
      "File: 1680.png, Predicted Class: 1\n",
      "File: 1681.png, Predicted Class: 0\n",
      "File: 1682.png, Predicted Class: 1\n",
      "File: 1683.png, Predicted Class: 0\n",
      "File: 1684.png, Predicted Class: 0\n",
      "File: 1685.png, Predicted Class: 1\n",
      "File: 1686.png, Predicted Class: 0\n",
      "File: 1687.png, Predicted Class: 1\n",
      "File: 1688.png, Predicted Class: 1\n",
      "File: 1689.png, Predicted Class: 0\n",
      "File: 1690.png, Predicted Class: 0\n",
      "File: 1691.png, Predicted Class: 2\n",
      "File: 1692.png, Predicted Class: 0\n",
      "File: 1693.png, Predicted Class: 0\n",
      "File: 1694.png, Predicted Class: 1\n",
      "File: 1695.png, Predicted Class: 0\n",
      "File: 1696.png, Predicted Class: 2\n",
      "File: 1697.png, Predicted Class: 1\n",
      "File: 1698.png, Predicted Class: 0\n",
      "File: 1699.png, Predicted Class: 1\n",
      "File: 1700.png, Predicted Class: 1\n",
      "File: 1701.png, Predicted Class: 1\n",
      "File: 1702.png, Predicted Class: 2\n",
      "File: 1703.png, Predicted Class: 0\n",
      "File: 1704.png, Predicted Class: 0\n",
      "File: 1705.png, Predicted Class: 2\n",
      "File: 1706.png, Predicted Class: 1\n",
      "File: 1707.png, Predicted Class: 2\n",
      "File: 1708.png, Predicted Class: 0\n",
      "File: 1709.png, Predicted Class: 2\n",
      "File: 1710.png, Predicted Class: 2\n",
      "File: 1711.png, Predicted Class: 0\n",
      "File: 1712.png, Predicted Class: 1\n",
      "File: 1713.png, Predicted Class: 1\n",
      "File: 1714.png, Predicted Class: 1\n",
      "File: 1715.png, Predicted Class: 2\n",
      "File: 1716.png, Predicted Class: 0\n",
      "File: 1717.png, Predicted Class: 0\n",
      "File: 1718.png, Predicted Class: 0\n",
      "File: 1719.png, Predicted Class: 0\n",
      "File: 1720.png, Predicted Class: 0\n",
      "File: 1721.png, Predicted Class: 2\n",
      "File: 1722.png, Predicted Class: 1\n",
      "File: 1723.png, Predicted Class: 1\n",
      "File: 1724.png, Predicted Class: 2\n",
      "File: 1725.png, Predicted Class: 0\n",
      "File: 1726.png, Predicted Class: 0\n",
      "File: 1727.png, Predicted Class: 0\n",
      "File: 1728.png, Predicted Class: 2\n",
      "File: 1729.png, Predicted Class: 2\n",
      "File: 1730.png, Predicted Class: 2\n",
      "File: 1731.png, Predicted Class: 0\n",
      "File: 1732.png, Predicted Class: 0\n",
      "File: 1733.png, Predicted Class: 2\n",
      "File: 1734.png, Predicted Class: 1\n",
      "File: 1735.png, Predicted Class: 1\n",
      "File: 1736.png, Predicted Class: 0\n",
      "File: 1737.png, Predicted Class: 0\n",
      "File: 1738.png, Predicted Class: 0\n",
      "File: 1739.png, Predicted Class: 1\n",
      "File: 1740.png, Predicted Class: 0\n",
      "File: 1741.png, Predicted Class: 1\n",
      "File: 1742.png, Predicted Class: 1\n",
      "File: 1743.png, Predicted Class: 2\n",
      "File: 1744.png, Predicted Class: 1\n",
      "File: 1745.png, Predicted Class: 1\n",
      "File: 1746.png, Predicted Class: 2\n",
      "File: 1747.png, Predicted Class: 1\n",
      "File: 1748.png, Predicted Class: 0\n",
      "File: 1749.png, Predicted Class: 0\n",
      "File: 1750.png, Predicted Class: 0\n",
      "File: 1751.png, Predicted Class: 2\n",
      "File: 1752.png, Predicted Class: 2\n",
      "File: 1753.png, Predicted Class: 0\n",
      "File: 1754.png, Predicted Class: 0\n",
      "File: 1755.png, Predicted Class: 1\n",
      "File: 1756.png, Predicted Class: 0\n",
      "File: 1757.png, Predicted Class: 0\n",
      "File: 1758.png, Predicted Class: 0\n",
      "File: 1759.png, Predicted Class: 0\n",
      "File: 1760.png, Predicted Class: 0\n",
      "File: 1761.png, Predicted Class: 1\n",
      "File: 1762.png, Predicted Class: 1\n",
      "File: 1763.png, Predicted Class: 0\n",
      "File: 1764.png, Predicted Class: 0\n",
      "File: 1765.png, Predicted Class: 0\n",
      "File: 1766.png, Predicted Class: 0\n",
      "File: 1767.png, Predicted Class: 2\n",
      "File: 1768.png, Predicted Class: 2\n",
      "File: 1769.png, Predicted Class: 2\n",
      "File: 1770.png, Predicted Class: 2\n",
      "File: 1771.png, Predicted Class: 0\n",
      "File: 1772.png, Predicted Class: 1\n",
      "File: 1773.png, Predicted Class: 0\n",
      "File: 1774.png, Predicted Class: 1\n",
      "File: 1775.png, Predicted Class: 1\n",
      "File: 1776.png, Predicted Class: 1\n",
      "File: 1777.png, Predicted Class: 1\n",
      "File: 1778.png, Predicted Class: 0\n",
      "File: 1779.png, Predicted Class: 1\n",
      "File: 1780.png, Predicted Class: 2\n",
      "File: 1781.png, Predicted Class: 1\n",
      "File: 1782.png, Predicted Class: 1\n",
      "File: 1783.png, Predicted Class: 0\n",
      "File: 1784.png, Predicted Class: 1\n",
      "File: 1785.png, Predicted Class: 0\n",
      "File: 1786.png, Predicted Class: 0\n",
      "File: 1787.png, Predicted Class: 2\n",
      "File: 1788.png, Predicted Class: 1\n",
      "File: 1789.png, Predicted Class: 0\n",
      "File: 1790.png, Predicted Class: 1\n",
      "File: 1791.png, Predicted Class: 1\n",
      "File: 1792.png, Predicted Class: 1\n",
      "File: 1793.png, Predicted Class: 0\n",
      "File: 1794.png, Predicted Class: 2\n",
      "File: 1795.png, Predicted Class: 1\n",
      "File: 1796.png, Predicted Class: 1\n",
      "File: 1797.png, Predicted Class: 0\n",
      "File: 1798.png, Predicted Class: 1\n",
      "File: 1799.png, Predicted Class: 1\n",
      "File: 1800.png, Predicted Class: 1\n",
      "File: 1801.png, Predicted Class: 0\n",
      "File: 1802.png, Predicted Class: 1\n",
      "File: 1803.png, Predicted Class: 2\n",
      "File: 1804.png, Predicted Class: 2\n",
      "File: 1805.png, Predicted Class: 0\n",
      "File: 1806.png, Predicted Class: 0\n",
      "File: 1807.png, Predicted Class: 0\n",
      "File: 1808.png, Predicted Class: 0\n",
      "File: 1809.png, Predicted Class: 0\n",
      "File: 1810.png, Predicted Class: 0\n",
      "File: 1811.png, Predicted Class: 0\n",
      "File: 1812.png, Predicted Class: 1\n",
      "File: 1813.png, Predicted Class: 0\n",
      "File: 1814.png, Predicted Class: 1\n",
      "File: 1815.png, Predicted Class: 1\n",
      "File: 1816.png, Predicted Class: 0\n",
      "File: 1817.png, Predicted Class: 2\n",
      "File: 1818.png, Predicted Class: 0\n",
      "File: 1819.png, Predicted Class: 2\n",
      "File: 1820.png, Predicted Class: 2\n",
      "File: 1821.png, Predicted Class: 0\n",
      "File: 1822.png, Predicted Class: 2\n",
      "File: 1823.png, Predicted Class: 1\n",
      "File: 1824.png, Predicted Class: 0\n",
      "File: 1825.png, Predicted Class: 1\n",
      "File: 1826.png, Predicted Class: 0\n",
      "File: 1827.png, Predicted Class: 1\n",
      "File: 1828.png, Predicted Class: 1\n",
      "File: 1829.png, Predicted Class: 0\n",
      "File: 1830.png, Predicted Class: 0\n",
      "File: 1831.png, Predicted Class: 0\n",
      "File: 1832.png, Predicted Class: 1\n",
      "File: 1833.png, Predicted Class: 1\n",
      "File: 1834.png, Predicted Class: 0\n",
      "File: 1835.png, Predicted Class: 0\n",
      "File: 1836.png, Predicted Class: 1\n",
      "File: 1837.png, Predicted Class: 0\n",
      "File: 1838.png, Predicted Class: 0\n",
      "File: 1839.png, Predicted Class: 0\n",
      "File: 1840.png, Predicted Class: 0\n",
      "File: 1841.png, Predicted Class: 1\n",
      "File: 1842.png, Predicted Class: 0\n",
      "File: 1843.png, Predicted Class: 0\n",
      "File: 1844.png, Predicted Class: 0\n",
      "File: 1845.png, Predicted Class: 1\n",
      "File: 1846.png, Predicted Class: 1\n",
      "File: 1847.png, Predicted Class: 0\n",
      "File: 1848.png, Predicted Class: 0\n",
      "File: 1849.png, Predicted Class: 1\n",
      "File: 1850.png, Predicted Class: 0\n",
      "File: 1851.png, Predicted Class: 2\n",
      "File: 1852.png, Predicted Class: 1\n",
      "File: 1853.png, Predicted Class: 2\n",
      "File: 1854.png, Predicted Class: 1\n",
      "File: 1855.png, Predicted Class: 1\n",
      "File: 1856.png, Predicted Class: 1\n",
      "File: 1857.png, Predicted Class: 1\n",
      "File: 1858.png, Predicted Class: 1\n",
      "File: 1859.png, Predicted Class: 1\n",
      "File: 1860.png, Predicted Class: 0\n",
      "File: 1861.png, Predicted Class: 0\n",
      "File: 1862.png, Predicted Class: 0\n",
      "File: 1863.png, Predicted Class: 0\n",
      "File: 1864.png, Predicted Class: 0\n",
      "File: 1865.png, Predicted Class: 0\n",
      "File: 1866.png, Predicted Class: 1\n",
      "File: 1867.png, Predicted Class: 2\n",
      "File: 1868.png, Predicted Class: 0\n",
      "File: 1869.png, Predicted Class: 0\n",
      "File: 1870.png, Predicted Class: 0\n",
      "File: 1871.png, Predicted Class: 0\n",
      "File: 1872.png, Predicted Class: 2\n",
      "File: 1873.png, Predicted Class: 1\n",
      "File: 1874.png, Predicted Class: 0\n",
      "File: 1875.png, Predicted Class: 1\n",
      "File: 1876.png, Predicted Class: 0\n",
      "File: 1877.png, Predicted Class: 2\n",
      "File: 1878.png, Predicted Class: 1\n",
      "File: 1879.png, Predicted Class: 0\n",
      "File: 1880.png, Predicted Class: 0\n",
      "File: 1881.png, Predicted Class: 1\n",
      "File: 1882.png, Predicted Class: 2\n",
      "File: 1883.png, Predicted Class: 0\n",
      "File: 1884.png, Predicted Class: 1\n",
      "File: 1885.png, Predicted Class: 2\n",
      "File: 1886.png, Predicted Class: 2\n",
      "File: 1887.png, Predicted Class: 1\n",
      "File: 1888.png, Predicted Class: 0\n",
      "File: 1889.png, Predicted Class: 1\n",
      "File: 1890.png, Predicted Class: 2\n",
      "File: 1891.png, Predicted Class: 0\n",
      "File: 1892.png, Predicted Class: 2\n",
      "File: 1893.png, Predicted Class: 0\n",
      "File: 1894.png, Predicted Class: 0\n",
      "File: 1895.png, Predicted Class: 0\n",
      "File: 1896.png, Predicted Class: 1\n",
      "File: 1897.png, Predicted Class: 0\n",
      "File: 1898.png, Predicted Class: 0\n",
      "File: 1899.png, Predicted Class: 0\n",
      "File: 1900.png, Predicted Class: 0\n",
      "File: 1901.png, Predicted Class: 0\n",
      "File: 1902.png, Predicted Class: 0\n",
      "File: 1903.png, Predicted Class: 0\n",
      "File: 1904.png, Predicted Class: 0\n",
      "File: 1905.png, Predicted Class: 0\n",
      "File: 1906.png, Predicted Class: 0\n",
      "File: 1907.png, Predicted Class: 0\n",
      "File: 1908.png, Predicted Class: 0\n",
      "File: 1909.png, Predicted Class: 2\n",
      "File: 1910.png, Predicted Class: 2\n",
      "File: 1911.png, Predicted Class: 2\n",
      "File: 1912.png, Predicted Class: 1\n",
      "File: 1913.png, Predicted Class: 1\n",
      "File: 1914.png, Predicted Class: 1\n",
      "File: 1915.png, Predicted Class: 1\n",
      "File: 1916.png, Predicted Class: 0\n",
      "File: 1917.png, Predicted Class: 0\n",
      "File: 1918.png, Predicted Class: 1\n",
      "File: 1919.png, Predicted Class: 0\n",
      "File: 1920.png, Predicted Class: 0\n",
      "File: 1921.png, Predicted Class: 2\n",
      "File: 1922.png, Predicted Class: 1\n",
      "File: 1923.png, Predicted Class: 1\n",
      "File: 1924.png, Predicted Class: 0\n",
      "File: 1925.png, Predicted Class: 1\n",
      "File: 1926.png, Predicted Class: 0\n",
      "File: 1927.png, Predicted Class: 0\n",
      "File: 1928.png, Predicted Class: 0\n",
      "File: 1929.png, Predicted Class: 0\n",
      "File: 1930.png, Predicted Class: 2\n",
      "File: 1931.png, Predicted Class: 2\n",
      "File: 1932.png, Predicted Class: 0\n",
      "File: 1933.png, Predicted Class: 0\n",
      "File: 1934.png, Predicted Class: 2\n",
      "File: 1935.png, Predicted Class: 0\n",
      "File: 1936.png, Predicted Class: 2\n",
      "File: 1937.png, Predicted Class: 1\n",
      "File: 1938.png, Predicted Class: 2\n",
      "File: 1939.png, Predicted Class: 1\n",
      "File: 1940.png, Predicted Class: 0\n",
      "File: 1941.png, Predicted Class: 0\n",
      "File: 1942.png, Predicted Class: 0\n",
      "File: 1943.png, Predicted Class: 0\n",
      "File: 1944.png, Predicted Class: 0\n",
      "File: 1945.png, Predicted Class: 2\n",
      "File: 1946.png, Predicted Class: 2\n",
      "File: 1947.png, Predicted Class: 1\n",
      "File: 1948.png, Predicted Class: 0\n",
      "File: 1949.png, Predicted Class: 1\n",
      "File: 1950.png, Predicted Class: 0\n",
      "File: 1951.png, Predicted Class: 0\n",
      "File: 1952.png, Predicted Class: 2\n",
      "File: 1953.png, Predicted Class: 1\n",
      "File: 1954.png, Predicted Class: 0\n",
      "File: 1955.png, Predicted Class: 0\n",
      "File: 1956.png, Predicted Class: 2\n",
      "File: 1957.png, Predicted Class: 2\n",
      "File: 1958.png, Predicted Class: 1\n",
      "File: 1959.png, Predicted Class: 0\n",
      "File: 1960.png, Predicted Class: 1\n",
      "File: 1961.png, Predicted Class: 0\n",
      "File: 1962.png, Predicted Class: 1\n",
      "File: 1963.png, Predicted Class: 1\n",
      "File: 1964.png, Predicted Class: 0\n",
      "File: 1965.png, Predicted Class: 1\n",
      "File: 1966.png, Predicted Class: 1\n",
      "File: 1967.png, Predicted Class: 0\n",
      "File: 1968.png, Predicted Class: 0\n",
      "File: 1969.png, Predicted Class: 1\n",
      "File: 1970.png, Predicted Class: 1\n",
      "File: 1971.png, Predicted Class: 0\n",
      "File: 1972.png, Predicted Class: 0\n",
      "File: 1973.png, Predicted Class: 1\n",
      "File: 1974.png, Predicted Class: 1\n",
      "File: 1975.png, Predicted Class: 2\n",
      "File: 1976.png, Predicted Class: 0\n",
      "File: 1977.png, Predicted Class: 1\n",
      "File: 1978.png, Predicted Class: 0\n",
      "File: 1979.png, Predicted Class: 0\n",
      "File: 1980.png, Predicted Class: 0\n",
      "File: 1981.png, Predicted Class: 0\n",
      "File: 1982.png, Predicted Class: 0\n",
      "File: 1983.png, Predicted Class: 2\n",
      "File: 1984.png, Predicted Class: 2\n",
      "File: 1985.png, Predicted Class: 1\n",
      "File: 1986.png, Predicted Class: 0\n",
      "File: 1987.png, Predicted Class: 0\n",
      "File: 1988.png, Predicted Class: 0\n",
      "File: 1989.png, Predicted Class: 0\n",
      "File: 1990.png, Predicted Class: 2\n",
      "File: 1991.png, Predicted Class: 2\n",
      "File: 1992.png, Predicted Class: 1\n",
      "File: 1993.png, Predicted Class: 2\n",
      "File: 1994.png, Predicted Class: 0\n",
      "File: 1995.png, Predicted Class: 0\n",
      "File: 1996.png, Predicted Class: 0\n",
      "File: 1997.png, Predicted Class: 1\n",
      "File: 1998.png, Predicted Class: 1\n",
      "File: 1999.png, Predicted Class: 0\n",
      "File: 2000.png, Predicted Class: 1\n",
      "File: 81_12_1.png, Predicted Class: 0\n",
      "File: 82_11_1.png, Predicted Class: 0\n",
      "File: 83_10.5_1.png, Predicted Class: 2\n",
      "File: 84_9.3_0.png, Predicted Class: 1\n",
      "File: 85_8.9_0.png, Predicted Class: 1\n",
      "File: 86_12.2_1.png, Predicted Class: 0\n",
      "File: 87_10.5_0.png, Predicted Class: 1\n",
      "File: 88_9.8_0.png, Predicted Class: 2\n",
      "File: 89_9.5_0.png, Predicted Class: 2\n",
      "File: 90_10.6_1.png, Predicted Class: 1\n",
      "File: 91_10.3_1.png, Predicted Class: 0\n",
      "File: 92_11_1.png, Predicted Class: 0\n",
      "File: 93.png, Predicted Class: 0\n",
      "File: 94.png, Predicted Class: 0\n",
      "File: 95.png, Predicted Class: 1\n",
      "File: 96.png, Predicted Class: 1\n",
      "File: 97.png, Predicted Class: 1\n",
      "File: 98.png, Predicted Class: 1\n",
      "File: 99.png, Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class AbhiNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3):\n",
    "        super(AbhiNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(256 * block.expansion, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def AbhiNet(num_classes=3):\n",
    "    return AbhiNet(BasicBlock, [2, 2, 2], num_classes)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_files[idx]\n",
    "\n",
    "data_dir_train = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Dog_heart\\\\Dog_heart\\\\Train'\n",
    "data_dir_val = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Dog_heart\\\\Dog_heart\\\\Valid'\n",
    "data_dir_test = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Test\\\\Test'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((75, 75)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=data_dir_train, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=data_dir_val, transform=transform)\n",
    "test_dataset = CustomDataset(root_dir=data_dir_test, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AbhiNet(num_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_weights_path = 'best_model_weights.pth'\n",
    "best_acc = 0.0\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    if average_val_loss < best_loss:\n",
    "        best_loss = average_val_loss\n",
    "        torch.save(model.state_dict(), best_weights_path)\n",
    "        print(f\"Saving best weights at epoch {epoch+1}\")\n",
    "\n",
    "print(f\"Training complete. Best validation loss: {best_loss:.4f}\")\n",
    "\n",
    "model.load_state_dict(torch.load(best_weights_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, file_names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for file_name, prediction in zip(file_names, predicted):\n",
    "            print(f\"File: {file_name}, Predicted Class: {prediction.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4414881",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.6295877546072006\n",
      "Epoch [1/20], Validation Loss: 0.6479151887553078, Accuracy: 66.5%\n",
      "Saving best weights at epoch 1\n",
      "Epoch [2/20], Training Loss: 0.6102675666863268\n",
      "Epoch [2/20], Validation Loss: 0.6144606641360691, Accuracy: 67.5%\n",
      "Saving best weights at epoch 2\n",
      "Epoch [3/20], Training Loss: 0.6011530201543461\n",
      "Epoch [3/20], Validation Loss: 0.663720863206046, Accuracy: 67.5%\n",
      "Epoch [4/20], Training Loss: 0.5751044709574092\n",
      "Epoch [4/20], Validation Loss: 0.6215676707880837, Accuracy: 70.5%\n",
      "Epoch [5/20], Training Loss: 0.5536233274774118\n",
      "Epoch [5/20], Validation Loss: 0.620386574949537, Accuracy: 69.0%\n",
      "Epoch [6/20], Training Loss: 0.5382494669068943\n",
      "Epoch [6/20], Validation Loss: 0.6182705504553658, Accuracy: 67.0%\n",
      "Epoch [7/20], Training Loss: 0.5093785565007817\n",
      "Epoch [7/20], Validation Loss: 0.6005680518490928, Accuracy: 68.0%\n",
      "Saving best weights at epoch 7\n",
      "Epoch [8/20], Training Loss: 0.49351356449452316\n",
      "Epoch [8/20], Validation Loss: 0.6295782455376217, Accuracy: 72.0%\n",
      "Epoch [9/20], Training Loss: 0.48800415071574127\n",
      "Epoch [9/20], Validation Loss: 0.5583186149597168, Accuracy: 72.0%\n",
      "Saving best weights at epoch 9\n",
      "Epoch [10/20], Training Loss: 0.46608006344600156\n",
      "Epoch [10/20], Validation Loss: 0.5885997499738421, Accuracy: 75.0%\n",
      "Epoch [11/20], Training Loss: 0.4687104970216751\n",
      "Epoch [11/20], Validation Loss: 0.6236536630562374, Accuracy: 72.5%\n",
      "Epoch [12/20], Training Loss: 0.46866230395707215\n",
      "Epoch [12/20], Validation Loss: 0.592903237257685, Accuracy: 72.0%\n",
      "Epoch [13/20], Training Loss: 0.4601300223307176\n",
      "Epoch [13/20], Validation Loss: 0.5872636309691838, Accuracy: 73.0%\n",
      "Epoch [14/20], Training Loss: 0.42685711180621927\n",
      "Epoch [14/20], Validation Loss: 0.6055436389786857, Accuracy: 74.5%\n",
      "Epoch [15/20], Training Loss: 0.4323015639727766\n",
      "Epoch [15/20], Validation Loss: 0.6108183988503048, Accuracy: 74.5%\n",
      "Epoch [16/20], Training Loss: 0.4340191541070288\n",
      "Epoch [16/20], Validation Loss: 0.6073326340743473, Accuracy: 74.5%\n",
      "Epoch [17/20], Training Loss: 0.4402352557940917\n",
      "Epoch [17/20], Validation Loss: 0.6080374079091209, Accuracy: 74.5%\n",
      "Epoch [18/20], Training Loss: 0.42931240322914993\n",
      "Epoch [18/20], Validation Loss: 0.6086850357907159, Accuracy: 75.0%\n",
      "Epoch [19/20], Training Loss: 0.4281491420485757\n",
      "Epoch [19/20], Validation Loss: 0.6143791867153985, Accuracy: 75.0%\n",
      "Epoch [20/20], Training Loss: 0.42609187351031735\n",
      "Epoch [20/20], Validation Loss: 0.6040519241775785, Accuracy: 75.5%\n",
      "Training complete. Best validation loss: 0.5583186149597168\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define data directories\n",
    "data_dir_train = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Dog_heart\\\\Dog_heart\\\\Train'\n",
    "data_dir_val = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Dog_heart\\\\Dog_heart\\\\Valid'\n",
    "data_dir_test = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Test\\\\Test'\n",
    "\n",
    "# Define transforms for resizing and normalizing images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((75, 75)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=data_dir_train, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=data_dir_val, transform=transform)\n",
    "# test_dataset = datasets.ImageFolder(root=data_dir_test, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "# model = AbhiNet(num_classes=3)  # Ensure your model is defined and imported\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Initialize variables for tracking the best validation loss and corresponding model weights\n",
    "best_loss = float('inf')\n",
    "best_weights_path = 'best_model_weights.pth'\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()  # Update learning rate\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss}, Accuracy: {acc}%\")\n",
    "\n",
    "        # Save the model if validation loss has decreased\n",
    "        if average_val_loss < best_loss:\n",
    "            best_loss = average_val_loss\n",
    "            torch.save(model.state_dict(), best_weights_path)\n",
    "            print(f\"Saving best weights at epoch {epoch+1}\")\n",
    "\n",
    "print(f\"Training complete. Best validation loss: {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3. Evaluate your model using the developed software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687038bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0, 0, 2, 2, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1, 2, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 2, 1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "Filenames: ['100.png', '1621.png', '1622.png', '1623.png', '1624.png', '1625.png', '1626.png', '1627.png', '1628.png', '1629.png', '1630.png', '1631.png', '1632.png', '1633.png', '1634.png', '1635.png', '1636.png', '1637.png', '1638.png', '1639.png', '1640.png', '1641.png', '1642.png', '1643.png', '1644.png', '1645.png', '1646.png', '1647.png', '1648.png', '1649.png', '1650.png', '1651.png', '1652.png', '1653.png', '1654.png', '1655.png', '1656.png', '1657.png', '1658.png', '1659.png', '1660.png', '1661.png', '1662.png', '1663.png', '1664.png', '1665.png', '1666.png', '1667.png', '1668.png', '1669.png', '1670.png', '1671.png', '1672.png', '1673.png', '1674.png', '1675.png', '1676.png', '1677.png', '1678.png', '1679.png', '1680.png', '1681.png', '1682.png', '1683.png', '1684.png', '1685.png', '1686.png', '1687.png', '1688.png', '1689.png', '1690.png', '1691.png', '1692.png', '1693.png', '1694.png', '1695.png', '1696.png', '1697.png', '1698.png', '1699.png', '1700.png', '1701.png', '1702.png', '1703.png', '1704.png', '1705.png', '1706.png', '1707.png', '1708.png', '1709.png', '1710.png', '1711.png', '1712.png', '1713.png', '1714.png', '1715.png', '1716.png', '1717.png', '1718.png', '1719.png', '1720.png', '1721.png', '1722.png', '1723.png', '1724.png', '1725.png', '1726.png', '1727.png', '1728.png', '1729.png', '1730.png', '1731.png', '1732.png', '1733.png', '1734.png', '1735.png', '1736.png', '1737.png', '1738.png', '1739.png', '1740.png', '1741.png', '1742.png', '1743.png', '1744.png', '1745.png', '1746.png', '1747.png', '1748.png', '1749.png', '1750.png', '1751.png', '1752.png', '1753.png', '1754.png', '1755.png', '1756.png', '1757.png', '1758.png', '1759.png', '1760.png', '1761.png', '1762.png', '1763.png', '1764.png', '1765.png', '1766.png', '1767.png', '1768.png', '1769.png', '1770.png', '1771.png', '1772.png', '1773.png', '1774.png', '1775.png', '1776.png', '1777.png', '1778.png', '1779.png', '1780.png', '1781.png', '1782.png', '1783.png', '1784.png', '1785.png', '1786.png', '1787.png', '1788.png', '1789.png', '1790.png', '1791.png', '1792.png', '1793.png', '1794.png', '1795.png', '1796.png', '1797.png', '1798.png', '1799.png', '1800.png', '1801.png', '1802.png', '1803.png', '1804.png', '1805.png', '1806.png', '1807.png', '1808.png', '1809.png', '1810.png', '1811.png', '1812.png', '1813.png', '1814.png', '1815.png', '1816.png', '1817.png', '1818.png', '1819.png', '1820.png', '1821.png', '1822.png', '1823.png', '1824.png', '1825.png', '1826.png', '1827.png', '1828.png', '1829.png', '1830.png', '1831.png', '1832.png', '1833.png', '1834.png', '1835.png', '1836.png', '1837.png', '1838.png', '1839.png', '1840.png', '1841.png', '1842.png', '1843.png', '1844.png', '1845.png', '1846.png', '1847.png', '1848.png', '1849.png', '1850.png', '1851.png', '1852.png', '1853.png', '1854.png', '1855.png', '1856.png', '1857.png', '1858.png', '1859.png', '1860.png', '1861.png', '1862.png', '1863.png', '1864.png', '1865.png', '1866.png', '1867.png', '1868.png', '1869.png', '1870.png', '1871.png', '1872.png', '1873.png', '1874.png', '1875.png', '1876.png', '1877.png', '1878.png', '1879.png', '1880.png', '1881.png', '1882.png', '1883.png', '1884.png', '1885.png', '1886.png', '1887.png', '1888.png', '1889.png', '1890.png', '1891.png', '1892.png', '1893.png', '1894.png', '1895.png', '1896.png', '1897.png', '1898.png', '1899.png', '1900.png', '1901.png', '1902.png', '1903.png', '1904.png', '1905.png', '1906.png', '1907.png', '1908.png', '1909.png', '1910.png', '1911.png', '1912.png', '1913.png', '1914.png', '1915.png', '1916.png', '1917.png', '1918.png', '1919.png', '1920.png', '1921.png', '1922.png', '1923.png', '1924.png', '1925.png', '1926.png', '1927.png', '1928.png', '1929.png', '1930.png', '1931.png', '1932.png', '1933.png', '1934.png', '1935.png', '1936.png', '1937.png', '1938.png', '1939.png', '1940.png', '1941.png', '1942.png', '1943.png', '1944.png', '1945.png', '1946.png', '1947.png', '1948.png', '1949.png', '1950.png', '1951.png', '1952.png', '1953.png', '1954.png', '1955.png', '1956.png', '1957.png', '1958.png', '1959.png', '1960.png', '1961.png', '1962.png', '1963.png', '1964.png', '1965.png', '1966.png', '1967.png', '1968.png', '1969.png', '1970.png', '1971.png', '1972.png', '1973.png', '1974.png', '1975.png', '1976.png', '1977.png', '1978.png', '1979.png', '1980.png', '1981.png', '1982.png', '1983.png', '1984.png', '1985.png', '1986.png', '1987.png', '1988.png', '1989.png', '1990.png', '1991.png', '1992.png', '1993.png', '1994.png', '1995.png', '1996.png', '1997.png', '1998.png', '1999.png', '2000.png', '81_12_1.png', '82_11_1.png', '83_10.5_1.png', '84_9.3_0.png', '85_8.9_0.png', '86_12.2_1.png', '87_10.5_0.png', '88_9.8_0.png', '89_9.5_0.png', '90_10.6_1.png', '91_10.3_1.png', '92_11_1.png', '93.png', '94.png', '95.png', '96.png', '97.png', '98.png', '99.png']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the path to your test folder\n",
    "test_folder_path = 'C:\\\\Users\\\\srikr\\\\Downloads\\\\Dog_X_ray\\\\Dog_X_ray\\\\Test\\\\Test'\n",
    "\n",
    "# Define transforms for the test images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((75, 75)),  # Resize images if needed\n",
    "    transforms.ToTensor(),        # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize if needed\n",
    "])\n",
    "\n",
    "# Custom dataset class to load images without labels\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(folder_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]  # Get the filename directly\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name  # Return filename along with the transformed image\n",
    "\n",
    "# Create a dataset instance for the test folder\n",
    "test_dataset = CustomDataset(test_folder_path, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load the best weights\n",
    "model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize an empty list to store predictions and filenames\n",
    "predictions = []\n",
    "filenames = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for images, img_names in test_loader:\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Get the predicted class index (integer)\n",
    "    _, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    # Append the predicted class index to predictions list\n",
    "    predictions.append(predicted.item())\n",
    "    \n",
    "    # Append the filename (without path) to filenames list\n",
    "    filenames.append(img_names[0])  # Assuming batch_size=1, so img_names will have one element\n",
    "\n",
    "# Print the predictions and corresponding filenames\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Filenames:\", filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac621a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array to DataFrame with array name as column name\n",
    "df_nn = pd.DataFrame({'filenames': filenames ,'nn_predictions': predictions })\n",
    "\n",
    "df_nn.to_csv(r'C:\\Users\\srikr\\Downloads\\Dog_X_ray\\Dog_X_ray\\CNN_Abhi.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 70%"
   ]
  },
  {
   "attachments": {
    "image_cnn_abhi.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0b6c30f0",
   "metadata": {},
   "source": [
    "![image_cnn_abhi.jpg](attachment:image_cnn_abhi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687dc4b",
   "metadata": {},
   "source": [
    "https://github.com/mvsakrishna/CNN_AbhiNet/tree/main\n",
    "\n",
    "https://www.researchgate.net/publication/382109004_AbhiNet_A_Custom_Convolutional_Neural_Networks\n",
    "\n",
    "https://github.com/mvsakrishna/CNN_AbhiNet/blob/main/best_model_weights.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476372c",
   "metadata": {},
   "source": [
    "# 6. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "     > = 70 % -->10 points\n",
    "     < 50 % -->0 points\n",
    "     >= 50 % & < 70% --> 0.5 point/percent\n",
    "     \n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b7b3c",
   "metadata": {},
   "source": [
    "The AbhiNet model, a hybrid convolutional neural network architecture inspired by LeNet-5, AlexNet, VGG-16, ResNet, and DenseNet, has been evaluated on a dataset of dog heart images resized to 75x75 pixels. Our experiments aimed to assess its effectiveness in classifying various dog heart conditions and understanding its overall performance.\n",
    "\n",
    "## Performance Evaluation\n",
    "We achieved a peak accuracy of 75.5% on the test set, demonstrating AbhiNet's capability to learn and generalize patterns from the dog heart dataset. This performance is competitive considering the complexities and variations in cardiac images, highlighting the efficacy of combining diverse architectural components.\n",
    "\n",
    "## Comparison with Existing Architectures\n",
    "Compared to traditional CNN architectures such as LeNet-5 and AlexNet, AbhiNet incorporates deeper layers and skip connections akin to ResNet, enhancing its ability to learn hierarchical features and mitigate the vanishing gradient problem. The integration of dense connectivity patterns from DenseNet further enriches feature propagation across layers, promoting better feature reuse and parameter efficiency.\n",
    "\n",
    "## Robustness and Generalization\n",
    "AbhiNet's performance across training, validation, and test sets underscores its robustness in handling variations and noise within the dataset. The use of batch normalization and dropout layers contributes to stabilizing training, thereby enhancing generalization and reducing overfitting.\n",
    "\n",
    "## Limitations and Challenges\n",
    "Despite its promising results, AbhiNet faces challenges typical of deep learning models in medical imaging. The need for large-scale annotated datasets and computational resources remains a barrier to fully harnessing its potential. Moreover, interpreting the learned features and decision-making processes within the model remains a topic of ongoing research.\n",
    "\n",
    "## Future Directions\n",
    "Future research directions include exploring transfer learning techniques to adapt AbhiNet to other medical imaging tasks beyond dog heart classification. Investigating ensemble methods or hybrid architectures with attention mechanisms could further enhance its performance and interpretability in complex diagnostic scenarios.\n",
    "\n",
    "## Conclusion\n",
    "In conclusion, AbhiNet represents a significant advancement in leveraging hybrid CNN architectures for medical image analysis, specifically in dog heart diagnostics. Its integration of multiple architectural principles from prominent CNNs has shown promise in achieving competitive performance and robustness. As we continue to refine and expand its capabilities, AbhiNet holds potential for broader applications in medical imaging and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d27c13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
